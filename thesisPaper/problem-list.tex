\chapter{Example Parallel Problems}
\label{probs}

\section{Rationale}
Each of the following problems was selected to illustrate 
both the advantages in approaching different forms of exploitable concurrency with the notion of function rank, 
as well as to illustrate how extensions of these problems to higher dimensions can be done easily, safely, 
and in an automatically data parallel fashion.

To demonstrate this, the problems were chosen so as to illustrate different kinds of \textit{parallel design patterns}. 
\textit{Design Patterns} are general, reusable solutions to a common class of problems. % TODO cite? (from Wikipedia)
Software design patterns were first made popular for object-oriented programming through the work of the "Gang of Four," % TODO cite
and are now seeing interest for applications in parallel computing. \cite{mass} 
This research has chosen to focus on the pattern language developed by Mattson et. al. 
presented in their book, "Patterns for Parallel Programming." % TODO justify?
Using parallel design patterns to guide the choices and discussion of these problems 
should allow this limited selection of problems to better represent families of parallel problems.

Solutions for each of the problems below using the partial implementation of the Parallel-J system
are given in Appendix \ref{apsol}

\section{Calculating Pi with Numerical Integration}
\subsection{Description}
One method of calculating $\pi$ is find the area of the unit circle. 
To do this, one can choose to calculate the area of the top right quandrant of the unit circle in the Cartesian plane, 
then multiply this area by 4. 
This is done with the following equation:

\[\pi = \int_0^1 \frac{4}{1 + x^2} \; dx\]

One way to approximate this formula is to use a left-handed Riemann sum:

\[\frac{1}{n} \cdot \sum_{i=0}^n \frac{4}{1 + x^2} \]

where $x = \frac{i}{n}$

\subsection{Relevant Design Pattern}
This problem was chosen to represent the \textit{Task Parallelism} pattern, 
in which ``the problem is decomposed into a collection of tasks that can execute concurrently.'' \cite{mass}
In this problem, the concurrent tasks are calculating $\frac{4}{1 + x^2}$ for the different values of $x$. 
There is also a task dependency: the multiplication of $\frac{1}{n}$ and the sum of the values produced by each task 
cannot occur until all tasks have finished. 
Each individual task takes roughly the same amount of time, as well, 
meaning no sophisticated scheduling is required in order to exploit concurrency effectively for large $n$.

\subsection{Solutions}
\subsubsection{C with OpenMP}
A parallel solution to this problem using C with the OpenMP libraries is given below. 
It is based off of the solution given in Appendix A of Mattson et. al. \cite{mass}

\begin{singlespacing}
\begin{small}
\verbatiminput{num-int-par.c}
\end{small}
\end{singlespacing}

This solution parallelizes each of the iterations of the for loop 
and specifies both that $x$ is a temporary work variable and so should not be shared among threads, 
and that there is to be a reduction on the variable $sum$ with addition. 
Since the entire body of the for loop is parallelized, including the update of $sum$, 
the annotations notifying the compiler of the reduction and private work variable is critical; 
otherwise, there could very well be race conditions.

\subsubsection{J}
A sequential J solution to this problem is given below, 
with some definitions to facilitate a discussion 
of the behavior of a solution in the proposed Parallel-J. 
The operators \ttfamily p\_rank \normalfont and \ttfamily p\_insert \normalfont 
are defined as the normal rank and insert operators, respectively,  
but are included to demonstrate where parallelism would be exploited in a parallel implementation of J.


\begin{singlespacing}
\begin{small}
\verbatiminput{num_int_pi.ijs}
\end{small}
\end{singlespacing}

This solution works conceptually by taking the for loop from the previous C solution 
and expanding it into an array, generated by $xs$.
Operations which were done in the body of the C for loop 
are instead expressed as operations on the whole array. 
Because J is functional, and 
because it allows the programer to express collective rather than item-by-item operations, 
the need for the temporary work variable $x$ used in the previous solution, 
a potential source of error, is eliminated. 

Similiarly, whereas the C solution required 
protecting against race conditions when reducing on the $sum$ variable
by giving compiler annotations, 
in the J solution this was never a risk.
Either the $sum$ operation would be safely parallel with $/::$ or 
it would be sequential with $/$.

A solution using the Parallel-J system which takes exactly the same approach as the J program 
can be found in Section \ref{pjsPi}.

% TODO extension?

\section{Conway's ``Game of Life''}
\subsection{Description}
Conway's ``Game of Life''\cite{gol} is a celluar automaton which is ``played'' on an infinite or bounded 2-dimensional grid. 
Every location on this grid represents a cell, which is either alive or dead. 
At each iteration, every cell is updated by applying the following set of rules:

\begin{enumerate}
	\item If the chosen cell is dead, then
	\begin{enumerate}
		\item If this cell has exactly 3 neighbors that are alive, 
			this cell becomes alive on the next iteration
		\item Otherwise, the cell remains dead
	\end{enumerate}
	\item If the chosen cell is alive, then
	\begin{enumerate}
		\item If this cell has either 2 or 3 neighbors that are alive, 
			this cell remains alive on the next iteration
		\item Otherwise, this cell becomes dead on the next iteration
	\end{enumerate}
\end{enumerate}

It is often convenient to represent each cell with a numeric value 
that is 0 when the cell is dead, and 1 when the cell is alive. 
Then, to calculate the number of alive neighbors for each give cell, 
one need only sum the values of all of the neighbors.

\subsection{Relevant Design Patterns}
This problem was chosen to represent the \textit{Geometric Decomposition} design pattern, \cite{mass}
in which ``the algorithm [is] organized around a data structure that has been decomposed into concurrently updatable `chunks'.''
For the bounded version of this problem, 
the data structure is a regular rank 2 array with shape \textit{n, m}, 
and the concurrently updatable chunk can be any subregion of the grid, 
with the limiting case that each cell is a chunk. 
While there are no data dependencies in this problem, 
care must be taken when using an imperative approach, 
as cells must be updated by reading their neighbors' current, and not future, values.

\subsection{Solutions}
\subsubsection{C with OpenMP}
A fragment of a parallel solution in C with OpenMP for the Game of Life is given below. 
The solution was work done by the author for an undergraduate course in parallel computing.
The fragment contains the function which updates each cell, 
which is where most of the computational concurrency for the game of life is found. 
Notice the two arguments to the function, \ttfamily board\normalfont and \ttfamily new\_board\normalfont.
In the iteration loop (not shown), these parameters are swapped every iteration, 
solving the potential race condition mentioned above.

\begin{singlespacing}
\begin{small}
\verbatiminput{game-of-life-omp.c}
\end{small}
\end{singlespacing}

Parallelism is specified on each of the row elements, indexed by $i$.
Again, we see that a safeguard for the index $j$ is required in order to avoid race conditions. 
This safeguard, as well as the swapping back and forth of grids (which would be required in a sequential solution similiar to this), 
are changes in code which have to be made not because of the nature of the computation itself, 
butbecause of the imperative, low-level nature of the tools to express it.

\subsubsection{J}
Like above, the J code below includes only the fragment responsible for updating the cells of the Game of Life. 

\begin{singlespacing}
\begin{small}
\verbatiminput{game_of_life.ijs}
\end{small}
\end{singlespacing}

One important difference between this solution and the previous is 
the approach to calculating the number of neighbor cells which are alive.
Instead of using array indexing at an item by item level, 
the neighbors are represented as a rank 3 array of shifted versions of the original board.
E.g.,

\begin{singlespacing}
\begin{small}
\begin{verbatim}
   ] board =: 3 3 (reshape =: $) 0 1 1  0 1 0  1 0 0
0 1 1
0 1 0
1 0 0
   NB. The up-left, up, and up-right neighbors

   0 1 2 (from =: {) neighborArray board 
0 0 0
0 0 1
0 0 1

0 0 0
0 1 1
0 1 0

0 0 0
1 1 0
1 0 0
\end{verbatim}
\end{small}
\end{singlespacing}

This is a data parallel approach to the problem,
in that it now acts on sub-collections of the rank 3 array independently. 
This approach is a safer alternative to requiring 
that the location of the cell is known to evaluate its neighbors, 
which potentially causes race conditions on thread-local variables.
Similarly, instead of specifying a reduction on a work variable \ttfamily sum\normalfont,
the parallel insert operator parallelize the operation in an automatically safe fashion.

One objection to this approach would be the potentially dramatic increase 
of memory use in the J approach compared to C with OMP. 
This can be avoided by using index permutations, % TODO cite?
in which only one copy of the original rank 2 array is kept.
Instead of direct indexing into the rank 3 array, 
an index function is used to determine 
what the direct index into the rank 2 array would be.

A solution using the Parallel-J system which takes a similiar approach as the J program 
(but includes a non-J for loop for the iteration)
can be found in Section \ref{pjsGol}.

\section{Merge Sort}
\subsection{Description}
Conceptually, sorting an array using merge sort is fairly easy to describe.

\begin{itemize}
	\item If the array as fewer than 2 elements, it's sorted
	\item Otherwise, divide the array into two halves and sort each of these
	\item Take the two sorted halves and merge them such that the resulting array remains sorted.
\end{itemize}

To simplify the discussion of the problem, 
we will be concerned with sorting arrays 
whose lengths are of the form $2^n$, where $n$ is a positive integer.
This way the nature of the parallelism in the problem can be discussed 
without going into the detail of handling special cases.

\subsection{Relevant Design Patterns}
This problem was chosen to represent the ``Divide and Conquer'' design pattern, \cite{mass}
in which, like its sequential equivalent,
``the problem is solved by splitting it into a number of smaller sub-problems, 
solving them independently [concurrently], and merging the subsolutions.''
Normally, problems that fit this design pattern are difficult to approach with data parallelism.
Given the constraints mentioned above, however,
merge sort will always the same number of sub-problems each with the same size, 
allowing for a data parallel approach.

\subsection{Solutions}
\subsubsection{C with OpenMP}
A parallel merge sort in C with OpenMP is given below.
The solution, as well as the description of its workings, 
was taken from an article on parallelism in OpenMP and MPI\cite{mergeomp}.

\begin{singlespacing}
\begin{small}
\verbatiminput{mergeSort-omp.c}
\end{small}
\end{singlespacing}

The \ttfamily sections \normalfont compiler directive % TODO citeP{omp}
specifies that the following blocks of code (each begin with a \ttfamily section \normalfont directive) 
are to be executed independently of each other.

Like with the previoius C examples, special care must be taken with the shared variable \ttfamily temp \normalfont 
so that race conditions do not occur.
However, in this situation a similiar serial program would also have to take the same precautions, 
meaning this is more of a limitation of the C programming language itself, rather than a limitation of OpenMP.

\subsubsection{J}
\label{jmerge}
In the following J code, the function \ttfamily merge \normalfont and its helper functions were ommited.
The full solution can be found in Appendix \ref{jmfull}

\begin{singlespacing}
\begin{small}
\verbatiminput{mergeSort.ijs}
\end{small}
\end{singlespacing}

This solution takes advantage of the constraints on the length of the array (it must be an integral power of two), 
expressing the regularity of the sizes of sub-problems through a regular array 
whose rank grows as the number of sub-problems does.
For example: 

\begin{singlespacing}
\begin{small}
\begin{verbatim}
   ] vec8 =: (deal =: ?~) 8
0 5 1 2 3 4 7 6
   ] vec16 =: deal 16
11 8 3 10 12 1 15 9 4 13 7 14 6 5 2 0
   divide vec8
0 5
1 2

3 4
7 6
   divide vec16
11  8
 3 10

12  1
15  9


 4 13
 7 14

 6  5
 2  0
\end{verbatim}
\end{small}
\end{singlespacing}

Unlike the high level description of the problem given at the begining of this section (but equivalently), 
this solution first sorts each two item vectors, 
expressing the recursive base-case of the merge sort algorithm as a data parallel operation.
Then, vectors are repeatedly merged via an insert of the \ttfamily merge \normalfont function on the rank 2 items 
(which always consist of two vector elements).
Again, while repeated function application doesn't quite fall into the bounds of data parallelism, 
each merge operation on the rank 2 items is itself data parallel.
