\chapter{Conclusion}
\label{conc}

\section{Discussion of Results}
\label{discres}
There are several potential reasons for the underwhelming results for the Parallel-J solutions given in Section \ref{res}. 
Probably the most significant reason for these results is that the body of code being parallelized (see Appendix \ref{apjverb}) 
potentially creates a large number of permanent and temporary objects, 
meaning that there are multiple requests for memory from the Java run-time heap. 
For example, since this work was originally intended to be a parallel implementation of J, 
several wrapper classes for were created to hold the same kind kind of information the current version of J\cite{ioj} 
maintains about arrays and the primitive values, like reference counts and bit flags for type information, 
and so where in a normal Scala library there would have been operations on primitive values, 
in Paralell-J these operations were encumbered to work on instances of wrapper classes. 
The constant creation of new objects for every single integer was likely a bottleneck for performance.

Also, operations that took existing collections and turned them into collections of higher rank 
were implemented by copying whole collections, operating on them, 
then concatenating each sub-result to return the final result. 
This constant copying of sub-arrays could have been avoided through the use of \textit{index transformation} 
which have been described in Section \ref{repa}, potentially improving performance significantly 
by removing these unnecessary operations.
 
Additionally, the large number of wrapper classes for primitive values 
caused the size of the program to cause cache faults to slow down performance significantly. 
This was first discovered when analyzing the performance results in Section \ref{respi}, 
in which increasing the problem size by a factor of 10, from $10,000$ to $100,000$, 
caused the program's run time to increase by a factor of 100. 
The solutions in C with OpenMP, in contrast, did not display this behavior, 
and instead only began showing significant improvements from parallelism in the same problem 
at a problem sizes of $1,000,000$.

Finally, this research did not make use of the macros introduced by Scala 2.10\cite{scala210} 
because Scala 2.10 was released too late in the research's development to be incorporated.
These macros have the potential to improve performance 
by replacing at compile time source code expressions that appear to be acting on objects 
with lower-level expressions acting on primitives, thus avoiding new object creation.

\section{Future Research}
\subsection{Parallel Implementation of J}
This research would be useful for future work towards a parallel alternative of J.
Regardless of the language chosen for implementing a parallel J, 
researchers must still consider which language features to include first in their prototype (Section \ref{desp})
and must still understand the inherrently data parallel nature of the rank operator (Section \ref{fridp})
Also the forces that lead to the choice of Scala as the language 
for this research (also listed in Section \ref{desp}), 
should suggest to future researchers the kinds of language features desirable 
for implementing a parallel J. 
If future work is done in this area in Scala 
or another language with suitable programming paradigms and libraries 
for developing a parallel implementation of J, 
then this research should be used as a prototype 
to help guide development.

Alternatively, a future parallel implementation of J
could be done in the C programming language, based on the current implementation of J\cite{ioj}. 
In order to begin this approach, 
it is strongly recommended that researchers first 
familiarize themselves with Roger Hui's documentation, ``An Implementation of J.''
Both this documentation and the full source code (under open sources licenses)
are available freely on the J Programming Website.
In addition, it seems that there are at least two viable options in such an approach 
which are possibly not mutually exclusive.
Future researchers may use shared memory parallel environments
such as the OpenMP library 
to parallelize operations within a single instance of a parallel J.

Also, the current implementation of J already includes 
functionality to allow for multiple instances of J to be running in the same process 
without race conditions.
Therefore, it may be possible to approach future work using 
a distributed memory parallel environment, such as Open MPI\cite{mpi}.

\subsection{Sequential and Parallel Scala Library for Regular Arrays using Function Rank}
Another possible extension to the work presented here 
is the development of Scala library for arbitrary dimensional collections that uses function rank. 
This library would ideally support parallelism in much the same way 
Scala's current collections library does\cite{pc},
through conversions between sequential and parallel implementations of each collection type. 
However, it should be clear that even 
a purely sequential version of such a library would be useful 
for solving problems which requires operations on several different dimensions, 
or which are naturally expressed in a higher dimension than the original problem description.

At the time this research was conducted, 
the author was not aware of current work being done in 
generic and polytipic programming in Scala by Miles Sabin. 
In particular this work, which is collectively called ``Shapeless''\cite{shapeless}
, supports collections whose sizes are known statically. 
This functionality would be useful for future work in developing 
a library of collections whose dimensions are known statically, 
possibly granting some or all of the advantages give when discussing 
the Haskell library Repa \cite{dph} in Section \ref{repa}.
Combining these advantages with functions supporting function rank into a single Scala library
would lead to significant expressiveness and reduced boiler-plate code, 
and could lend itself to a future parallel implementation for good performance, as well.
