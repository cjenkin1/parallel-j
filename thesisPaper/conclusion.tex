\chapter{Conclusion}
\label{conc}

\section{Discussion of Results}
\label{discres}
There are several potential reasons for the underwhelming results for the Parallel-J solutions given in Section \ref{res}. 
Probably the most significant of these is that the body of code being parallelized (see Appendix \ref{apjverb}) 
potentially creates a large number of permanent and temporary objects, 
meaning that there are multiple requests for memory from the Java run-time heap. 
For example, since this work was originally intended to be a parallel implementation of J, 
several wrapper classes for were created to hold the same kind kind of information the current version of J\cite{ioj} 
maintains about arrays and the primitive values, like reference counts and bit flags for type information. 
The constant creation of new objects for every single integer was likely a bottleneck for performance.
Also, operations that took existing collections and turned them into collections of higher rank 
were implemented by copying whole collections, operating on them, 
then concatenating each sub-result to return the final result. 
This constant copying of sub-arrays could have been avoided through the use of \textit{index transformation} 
which have been described in Section \ref{repa}.
 
Additionally, the large number of wrapper classes for primitive values 
caused the size of the program to cause cache faults to slow down performance significantly. 
This was first discovered when analyzing the performance results in Section \ref{respi}, 
in which increasing the problem size by a factor of 10, from $10,000$ to $100,000$, 
caused the program's run time to increase by a factor of 100. 
The solutions in C with OpenMP, in contrast, did not display this behavior, 
and instead only began showing significant improvements from parallelism in the same problem 
at a problem sizes of $1,000,000$.

Finally, this research did not make use of the macros introduced by Scala 2.10\cite{scala210}.
These macros have the potential to improve performance 
by replacing at compile time source code expressions that appear to be acting on objects 
with lower-level expressions acting on primitives, thus avoiding new object creation.

\section{Future Research}
\subsection{Parallel Implementation of J}
This research could be useful for future work which provides 
a parallel alternative of J.
No matter the implementing language chosen, 
the considerations in Section \ref{desp} 
over which language features to include in a prototype, 
and the discussion in Section \ref{fridp} of the behavior and inherent data parallelism of the rank operator, 
would likely be useful to those not intimately familiar with the current J implementation.

Also to be taken into consideration should be 
the forces which lead to the choice of Scala as the language 
for this research, also listed in Section \ref{desp}. 
If future work is done in this area in Scala 
or another language with suitable programming paradigms and libraries 
for developing a parallel implementation of J, 
then this research should be used as a prototype 
to help guide development.

Alternatively, a future parallel implementation of J
could be done in the C programming language. 
In order to begin this approach, 
it is strongly recommended that researchers first 
familiarize themselves with the working of the current implementation of J\cite{ioj}, 
the source code and documentation of which 
is available freely under open source licenses.
In addition, it seems that there are at least two viable options in such an approach 
which are possibly not mutually exclusive.
Future researchers may use shared memory parallel environments
such as the OpenMP library 
to parallelize operations within a single instance of a parallel J.

Also, the current implementation of J already includes 
functionality to allow for multiple instances of J to be running in the same process 
without race conditions.
Therefore, it may be possible to approach future work using 
a distributed memory parallel environment, such as MPI. % TODO cite!

\subsection{Sequential and Parallel Scala Library for Regular Arrays using Function Rank}
Another possible extension to the work presented here 
is the development of Scala library for arbitrary dimensional collections which uses function rank. 
This library would ideally support parallelism in much the same way 
Scala's current collections library does\cite{pc},
through conversions between sequential and parallel implementations of each collection type. 
However, it should be clear that even 
a purely sequential version of such a library would be useful 
for solving problems which requires operations on several different dimensions, 
or which are naturally expressed in a higher dimension than the original problem description.

At the time this research was conducted, 
the author was not aware of current work being done in 
generic and polytipic programming in Scala by Miles Sabin. 
In particular this work, which is collectively called ``Shapeless''%TODO \cite{shapeless}
, supports collections whose sizes are known statically. 
This functionality would be useful for future work in developing 
a library of collections whose dimensions are known statically, 
possibly granting some or all of the advantages give when discussing 
the Haskell library Repa \cite{dph} in Section \ref{repa}.
Combining these advantages with functions supporting function rank into a single Scala library
would lead to significant expressiveness and reduced boiler-plate code, 
and could lend itself to a future parallel implementation for good performance, as well.
